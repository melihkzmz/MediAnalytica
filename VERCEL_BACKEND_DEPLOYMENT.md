# ğŸš€ Vercel Backend Deployment Guide

## Can Vercel Deploy Flask Backend?

**Short Answer**: **Yes, but with significant limitations** âš ï¸

Vercel **CAN** run Python/Flask applications, but it's **NOT ideal** for your use case (ML model inference).

---

## âœ… What Vercel CAN Do

### 1. **Serverless Functions**
- âœ… Run Python serverless functions
- âœ… Handle HTTP requests
- âœ… Support Flask (with adaptation)
- âœ… Auto-scaling
- âœ… Global edge network

### 2. **Python Runtime**
- âœ… Python 3.9, 3.10, 3.11
- âœ… Install packages via `requirements.txt`
- âœ… Environment variables support

---

## âŒ Critical Limitations for Your Project

### 1. **Function Timeout**
- âš ï¸ **Hobby Plan**: 10 seconds max
- âš ï¸ **Pro Plan**: 60 seconds max
- âš ï¸ **Enterprise**: Up to 300 seconds

**Your ML models** (skin, bone, lung, eye disease prediction) may take longer than 10-60 seconds, especially:
- Model loading time (cold start)
- Image preprocessing
- Model inference
- Grad-CAM visualization

### 2. **Memory Limits**
- âš ï¸ **Hobby**: 1GB RAM
- âš ï¸ **Pro**: 3GB RAM
- âš ï¸ **Enterprise**: Up to 10GB

**Your models are large**:
- `bone_4class_improved_finetuned.keras`: **93.99 MB**
- `skin_5class_efficientnetb3_macro_f1_finetuned.keras`: **74.22 MB**
- Multiple models loaded simultaneously = memory issues

### 3. **Cold Start Problem**
- âš ï¸ First request after inactivity: **5-30 seconds** to load model
- âš ï¸ Models need to be loaded into memory each time (if not cached)
- âš ï¸ Poor user experience for image analysis

### 4. **File Size Limits**
- âš ï¸ **Request body**: 4.5MB max (Hobby), 4.5MB (Pro)
- âš ï¸ **Function package**: 50MB (Hobby), 250MB (Pro)
- âš ï¸ Your models exceed these limits

### 5. **No Persistent Storage**
- âš ï¸ Stateless functions (can't keep models in memory between requests)
- âš ï¸ Need to reload models on each cold start

### 6. **No GPU Support**
- âš ï¸ CPU-only inference (slower)
- âš ï¸ No CUDA/GPU acceleration

---

## ğŸ¯ Better Alternatives for Your Backend

### **Option 1: Railway** â­ (Recommended)
- âœ… **Unlimited** execution time
- âœ… **8GB RAM** (free tier)
- âœ… **Persistent** processes (models stay loaded)
- âœ… **Easy deployment** (GitHub integration)
- âœ… **Free tier** available
- âœ… **Custom domains**
- âœ… **Environment variables**
- âœ… **Logs & monitoring**

**Perfect for**: Flask apps with ML models

### **Option 2: Render**
- âœ… **Unlimited** execution time
- âœ… **512MB RAM** (free tier), 2GB+ (paid)
- âœ… **Persistent** processes
- âœ… **Free tier** available
- âœ… **Auto-deploy from GitHub**

**Good for**: Flask apps, slightly less generous than Railway

### **Option 3: Fly.io**
- âœ… **Unlimited** execution time
- âœ… **256MB RAM** (free tier), scalable
- âœ… **Global edge** deployment
- âœ… **Free tier** available

**Good for**: Flask apps with global distribution

### **Option 4: Google Cloud Run / AWS Lambda**
- âœ… **Scalable** serverless
- âœ… **Higher** timeout limits
- âœ… **GPU support** available (paid)
- âš ï¸ More complex setup
- âš ï¸ Pay-per-use pricing

---

## ğŸ”§ How to Deploy Flask on Vercel (If You Still Want To)

### Step 1: Create `api/` Directory Structure

```
project-root/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ predict.py          # Serverless function
â”‚   â”œâ”€â”€ auth.py             # Auth endpoints
â”‚   â””â”€â”€ profile.py          # Profile endpoints
â”œâ”€â”€ vercel.json
â””â”€â”€ requirements.txt
```

### Step 2: Convert Flask Routes to Serverless Functions

**Example**: `api/predict.py`

```python
from vercel import Response
import tensorflow as tf
from PIL import Image
import io
import numpy as np

# Load model (cached between warm starts)
model = None

def load_model():
    global model
    if model is None:
        model = tf.keras.models.load_model('models/skin_model.keras')
    return model

def handler(request):
    if request.method != 'POST':
        return Response({'error': 'Method not allowed'}, status=405)
    
    # Get image from request
    image_file = request.files.get('image')
    if not image_file:
        return Response({'error': 'No image provided'}, status=400)
    
    # Load model (may take 5-10 seconds on cold start)
    model = load_model()
    
    # Preprocess image
    image = Image.open(io.BytesIO(image_file.read()))
    # ... preprocessing ...
    
    # Predict (may take 5-15 seconds)
    predictions = model.predict(preprocessed_image)
    
    return Response({
        'success': True,
        'predictions': predictions.tolist()
    })
```

### Step 3: Configure `vercel.json`

```json
{
  "version": 2,
  "builds": [
    {
      "src": "api/**/*.py",
      "use": "@vercel/python"
    }
  ],
  "routes": [
    {
      "src": "/api/(.*)",
      "dest": "/api/$1"
    }
  ],
  "functions": {
    "api/predict.py": {
      "maxDuration": 60
    }
  }
}
```

### Step 4: Limitations You'll Face

1. **Model Loading**: 5-30 seconds on cold start
2. **Inference Time**: May exceed 60 seconds
3. **Memory**: Large models may cause OOM errors
4. **Package Size**: Models may exceed 250MB limit

---

## ğŸ¯ Recommended Architecture

### **Hybrid Approach** (Best Solution)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚
â”‚   (Vercel)      â”‚  â† Static files, fast CDN
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ API Calls
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Backend       â”‚
â”‚   (Railway)     â”‚  â† Flask API, ML models
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Firebase      â”‚
â”‚   (Auth/DB)     â”‚  â† Authentication, Database
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why This Works Best:

1. **Vercel**: 
   - âœ… Fast static file delivery
   - âœ… Global CDN
   - âœ… Free tier
   - âœ… Easy deployment

2. **Railway/Render**:
   - âœ… Handles long-running ML inference
   - âœ… Persistent model loading
   - âœ… No timeout issues
   - âœ… Better for CPU-intensive tasks

3. **Firebase**:
   - âœ… Already integrated
   - âœ… Handles auth & database
   - âœ… No changes needed

---

## ğŸ“Š Comparison Table

| Feature | Vercel | Railway | Render | Fly.io |
|---------|--------|---------|--------|--------|
| **Max Timeout** | 10-60s | Unlimited | Unlimited | Unlimited |
| **Memory** | 1-3GB | 8GB (free) | 512MB-2GB | 256MB+ |
| **ML Models** | âŒ Limited | âœ… Good | âœ… Good | âœ… Good |
| **Cold Start** | âš ï¸ 5-30s | âœ… Minimal | âœ… Minimal | âœ… Minimal |
| **Free Tier** | âœ… Yes | âœ… Yes | âœ… Yes | âœ… Yes |
| **Ease of Use** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­ |
| **Best For** | Frontend | Backend | Backend | Backend |

---

## ğŸš€ Quick Start: Deploy Backend to Railway

### Step 1: Create `Procfile`

```
web: python Skin-Disease-Classifier/auth_api.py
```

### Step 2: Update `auth_api.py` Port

```python
import os
port = int(os.getenv('PORT', 5001))
app.run(host="0.0.0.0", port=port, debug=False)
```

### Step 3: Deploy to Railway

1. Go to [railway.app](https://railway.app)
2. Click "New Project"
3. Select "Deploy from GitHub"
4. Choose your repository
5. Railway auto-detects Python and installs dependencies
6. Add environment variables:
   - `FIREBASE_CREDENTIALS_PATH`
   - `PORT` (auto-set)
   - `FLASK_DEBUG=False`
   - `CORS_ORIGINS` (your Vercel URL)

### Step 4: Update Frontend Config

In `config.js`:
```javascript
apiUrl: window.location.hostname === 'localhost' 
    ? 'http://localhost:5001'
    : 'https://your-backend.railway.app'
```

---

## ğŸ’¡ Final Recommendation

### âœ… **DO THIS**:
1. **Frontend** â†’ Deploy to **Vercel** (perfect for static files)
2. **Backend** â†’ Deploy to **Railway** or **Render** (perfect for Flask + ML)
3. **Database/Auth** â†’ Keep on **Firebase** (already set up)

### âŒ **DON'T DO THIS**:
- âŒ Don't deploy Flask backend to Vercel (timeout/memory issues)
- âŒ Don't try to run ML models on Vercel serverless functions
- âŒ Don't use Vercel for long-running processes

---

## ğŸ“ Summary

**Question**: Can Vercel deploy Flask backend for image analysis?

**Answer**: 
- **Technically**: Yes, with serverless functions
- **Practically**: **No, not recommended** for ML workloads
- **Best Practice**: Use Vercel for frontend, Railway/Render for backend

Your ML models need:
- âœ… Unlimited execution time
- âœ… Persistent memory (models stay loaded)
- âœ… More RAM (8GB+ recommended)
- âœ… No cold start delays

**Vercel doesn't provide these for ML workloads**, but **Railway/Render do**! ğŸ¯

